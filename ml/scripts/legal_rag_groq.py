#!/usr/bin/env python3
"""
Legal RAG System with Groq API
Ultra-fast responses with proper Nepali formatting and strict context grounding
"""

import chromadb
from chromadb.utils import embedding_functions
from pathlib import Path
from typing import List, Dict, Any
import os
from dotenv import load_dotenv
from groq import Groq

class LegalRAGWithGroq:
    def __init__(self, base_dir: str = "D:/okil ai/ml"):
        self.base_dir = Path(base_dir)
        self.db_dir = self.base_dir / "embeddings" / "chroma_db_v2"
        
        # Load environment variables
        env_path = Path("D:/okil ai/.env")
        load_dotenv(env_path)
        
        # Get Groq API key
        self.groq_api_key = os.getenv("Groq_API_KEY")
        if not self.groq_api_key:
            raise ValueError("Groq_API_KEY not found in .env file")
        
        # Initialize Groq client
        self.client = Groq(api_key=self.groq_api_key)
        
        # Available models: 
        # - llama-3.3-70b-versatile (best quality, multilingual)
        # - llama-3.1-8b-instant (fast)
        # - mixtral-8x7b-32768 (good multilingual)
        # - gemma2-9b-it (fast, good for Nepali)
        self.model = "llama-3.3-70b-versatile"
        
        print("‚úì Groq API configured successfully")
        print(f"  Model: {self.model}")
        
        # Embedding Model
        self.embedding_model_name = "intfloat/multilingual-e5-large"
        self.collection_name = "legal_corpus_v2"
        
        # Initialize ChromaDB client
        print(f"Connecting to ChromaDB at {self.db_dir}...")
        self.client_db = chromadb.PersistentClient(path=str(self.db_dir))
        
        # Load embedding function
        print(f"Loading embedding model: {self.embedding_model_name}...")
        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=self.embedding_model_name,
            device="cpu"
        )
        
        # Get the collection
        try:
            self.collection = self.client_db.get_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function
            )
            print(f"‚úì Connected to collection '{self.collection_name}' with {self.collection.count()} documents.\n")
        except Exception as e:
            print(f"Error: Could not load collection. {e}")
            raise e

    def search(self, query: str, top_k: int = 6) -> List[Dict[str, Any]]:
        """Search for relevant legal text chunks."""
        print(f"üîç Searching for: '{query}'")
        
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k
        )
        
        formatted_results = []
        for i in range(len(results['ids'][0])):
            formatted_results.append({
                'rank': i + 1,
                'id': results['ids'][0][i],
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i] if 'distances' in results else None
            })
        
        print(f"‚úì Found {len(formatted_results)} relevant legal chunks.\n")
        return formatted_results

    def generate_system_prompt(self) -> str:
        """Generate system prompt that enforces strict context-only answers."""
        return """You are "‡§µ‡§ï‡§ø‡§≤" - an expert Nepali legal advisor AI assistant. You help users understand Nepali law by providing accurate, well-structured answers.

CRITICAL RULES (YOU MUST FOLLOW STRICTLY):

1. **Language**: Answer ONLY in NEPALI (‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§≠‡§æ‡§∑‡§æ), even if the question is in English

2. **Source Grounding**: Use ONLY the legal context provided in the user message. DO NOT use your training data or general knowledge. If the context doesn't contain the answer, clearly state: "‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§¶‡§ø‡§á‡§è‡§ï‡•ã ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠‡§Æ‡§æ ‡§Ø‡§∏ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§ï‡•ã ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ú‡§µ‡§æ‡§´ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§õ‡•à‡§®‡•§"

3. **Citations**: For every important legal point, provide clear citations:
   - Format: "‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®, ‡§ß‡§æ‡§∞‡§æ ‡•≠‡•¨ ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞..."
   - Or: "‡§®‡§æ‡§ó‡§∞‡§ø‡§ï ‡§∏‡§Ç‡§π‡§ø‡§§‡§æ, ‡•®‡•¶‡•≠‡•™, ‡§¶‡§´‡§æ ‡•ß‡•≠ ‡§¨‡§Æ‡•ã‡§ú‡§ú‡§Æ..."

4. **Professional Formatting**:
   - Start with a brief introduction
   - Use bullet points (‚Ä¢) or numbered lists (‡•ß., ‡•®., ‡•©.)
   - Use relevant emojis for clarity: ‚öñÔ∏è (law), üìã (rules), ‚úÖ (allowed), ‚ùå (prohibited), üìä (process)
   - Use headers with ** ** for sections
   - End with "üìö ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠:" section listing sources

5. **Structure Example**:
```
[Brief introduction answering the core question]

**‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§¨‡•Å‡§Å‡§¶‡§æ‡§π‡§∞‡•Ç:**

‡•ß. **[Topic 1]:** 
   ‚Ä¢ [Point with citation]
   ‚Ä¢ [Point with citation]

‡•®. **[Topic 2]:**
   ‚Ä¢ [Point with citation]

**‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ:** [If applicable]
‚Ä¢ [Step-by-step process]

üìö **‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠:**
‚Ä¢ ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®, ‡§ß‡§æ‡§∞‡§æ X
‚Ä¢ [Other sources used]
```

6. **Accuracy**: Be precise and detailed. Don't generalize or assume.

Remember: Your credibility depends on ONLY using the provided legal context. Never hallucinate or add information not in the context."""

    def generate_user_prompt(self, query: str, retrieved_chunks: List[Dict[str, Any]]) -> str:
        """Generate user prompt with query and legal context."""
        
        # Build the context from retrieved chunks
        context_parts = []
        for i, chunk in enumerate(retrieved_chunks, 1):
            citation_ref = chunk['metadata'].get('citation_reference', 'Unknown Source')
            section_type = chunk['metadata'].get('section_type_ne', '')
            section_num = chunk['metadata'].get('section_number', '')
            text = chunk['text']
            
            header = f"[‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠ {i}: {citation_ref}"
            if section_type and section_num:
                header += f", {section_type} {section_num}"
            header += "]"
            
            context_parts.append(f"{header}\n{text}")
        
        context = "\n\n---\n\n".join(context_parts)
        
        # Construct the user message
        user_message = f"""**‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ‡§ï‡•ã ‡§™‡•ç‡§∞‡§∂‡•ç‡§®:**
{query}

**‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠ (‡§Ø‡•ã ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç):**

{context}

---

‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§æ‡§•‡§ø‡§ï‡•ã ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠‡§ï‡•ã ‡§Ü‡§ß‡§æ‡§∞‡§Æ‡§æ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ‡§ï‡•ã ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§, ‡§∏‡§Ç‡§∞‡§ö‡§ø‡§§ ‡§∞ ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§ú‡§µ‡§æ‡§´ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä‡§Æ‡§æ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§"""
        
        return user_message

    def query_groq(self, system_prompt: str, user_prompt: str) -> str:
        """Send prompt to Groq and get streaming response."""
        
        print("ü§ñ Generating answer using Groq (ultra-fast)...")
        print("=" * 60)
        print("üìñ ‡§ú‡§µ‡§æ‡§´ (‡§µ‡§ï‡§ø‡§≤):")
        print("=" * 60 + "\n")
        
        try:
            # Create chat completion with streaming
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,  # Low temperature for factual accuracy
                max_tokens=2048,
                top_p=0.9,
                stream=True
            )
            
            full_response = ""
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    print(content, end='', flush=True)
                    full_response += content
            
            print("\n\n" + "=" * 60)
            return full_response
                
        except Exception as e:
            error_msg = f"‚ùå Error with Groq API: {str(e)}"
            print(error_msg)
            return error_msg

    def answer_question(self, query: str, top_k: int = 6) -> Dict[str, Any]:
        """Complete RAG pipeline: retrieve relevant chunks and generate answer."""
        
        # Step 1: Retrieve relevant chunks
        results = self.search(query, top_k=top_k)
        
        if not results:
            return {
                'query': query,
                'answer': '‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§∏‡§Å‡§ó ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡§ø‡§§ ‡§ï‡•Å‡§®‡•à ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§',
                'sources': []
            }
        
        # Step 2: Generate prompts
        system_prompt = self.generate_system_prompt()
        user_prompt = self.generate_user_prompt(query, results)
        
        # Step 3: Get answer from Groq
        answer = self.query_groq(system_prompt, user_prompt)
        
        # Step 4: Display sources
        print(f"\nüìö ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡§ø‡§è‡§ï‡§æ ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§∏‡•ç‡§∞‡•ã‡§§‡§π‡§∞‡•Ç ({len(results)} ‡§Ö‡§Ç‡§∂):")
        print("-" * 60)
        for i, source in enumerate(results, 1):
            citation = source['metadata'].get('citation_reference', 'Unknown')
            section_type = source['metadata'].get('section_type_ne', '')
            section_num = source['metadata'].get('section_number', '')
            if section_type and section_num:
                print(f"   {i}. {citation} ({section_type} {section_num})")
            else:
                print(f"   {i}. {citation}")
        print()
        
        return {
            'query': query,
            'answer': answer,
            'sources': results,
            'system_prompt': system_prompt,
            'user_prompt': user_prompt
        }


def main():
    """Interactive Q&A session with Groq."""
    print("=" * 60)
    print("‚ö° LEGAL RAG SYSTEM WITH GROQ API")
    print("=" * 60)
    print("‚ú® Ultra-fast responses (< 5 seconds)")
    print("‚ú® Cross-lingual: Ask in English, get Nepali answers")
    print("‚ú® Powered by: Llama 3.1 70B + ChromaDB")
    print("=" * 60 + "\n")
    
    try:
        # Initialize RAG system
        rag = LegalRAGWithGroq()
        
        print("üí° Example queries:")
        print("   ‚Ä¢ How is the prime minister elected in Nepal?")
        print("   ‚Ä¢ What are the fundamental rights?")
        print("   ‚Ä¢ ‡§ú‡§ó‡•ç‡§ó‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡•ã ‡§®‡§ø‡§Ø‡§Æ ‡§ï‡•á ‡§õ?")
        print("   ‚Ä¢ ‡§∏‡§Æ‡•ç‡§™‡§§‡•ç‡§§‡§ø ‡§ï‡§∞ ‡§ï‡§∏‡§∞‡•Ä ‡§§‡§ø‡§∞‡•ç‡§®‡•á?")
        print("   ‚Ä¢ ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§§‡§æ‡§ï‡•ã ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§ï‡•á ‡§õ‡§®‡•ç?")
        print("   ‚Ä¢ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø‡§ï‡•ã ‡§∂‡§ï‡•ç‡§§‡§ø ‡§∞ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§π‡•Å‡§®‡•ç?")
        
        print(f"\n‚úÖ Ready for questions! (Type 'quit' to exit)")
        print("=" * 60)
        
        while True:
            try:
                query = input("\n‚ùì Your question: ").strip()
                
                if query.lower() in ['quit', 'exit', 'q']:
                    print("\nüëã ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! ‡§´‡•á‡§∞‡§ø ‡§≠‡•á‡§ü‡•å‡§Ç‡§≤‡§æ‡•§")
                    break
                
                if not query:
                    continue
                
                # Get answer
                result = rag.answer_question(query, top_k=6)
                
            except KeyboardInterrupt:
                print("\n\nüëã ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! Goodbye!")
                break
            except Exception as e:
                print(f"\n‚ùå Error: {e}")
                import traceback
                traceback.print_exc()
    
    except Exception as e:
        print(f"Failed to initialize system: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
