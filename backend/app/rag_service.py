#!/usr/bin/env python3
"""
Legal RAG System with Groq API
Ultra-fast responses with proper Nepali formatting and strict context grounding
"""

import chromadb
from chromadb.utils import embedding_functions
from pathlib import Path
from typing import List, Dict, Any
import os
from dotenv import load_dotenv
from groq import Groq

class LegalRAGWithGroq:
    def __init__(self, base_dir: str = "D:/okil ai/ml"):
        self.base_dir = Path(base_dir)
        self.db_dir = self.base_dir / "embeddings" / "chroma_db_v2"
        
        # Load environment variables
        env_path = Path("D:/okil ai/.env")
        load_dotenv(env_path)
        
        # Get Groq API key
        self.groq_api_key = os.getenv("Groq_API_KEY")
        if not self.groq_api_key:
            raise ValueError("Groq_API_KEY not found in .env file")
        
        # Initialize Groq client
        self.client = Groq(api_key=self.groq_api_key)
        
        # Available models: 
        # - llama-3.3-70b-versatile (best quality, multilingual)
        # - llama-3.1-8b-instant (fast)
        # - mixtral-8x7b-32768 (good multilingual)
        # - gemma2-9b-it (fast, good for Nepali)
        self.model = "llama-3.3-70b-versatile"
        
        print("‚úì Groq API configured successfully")
        print(f"  Model: {self.model}")
        
        # Embedding Model
        self.embedding_model_name = "intfloat/multilingual-e5-large"
        self.collection_name = "legal_corpus_v2"
        
        # Initialize ChromaDB client
        print(f"Connecting to ChromaDB at {self.db_dir}...")
        self.client_db = chromadb.PersistentClient(path=str(self.db_dir))
        
        # Load embedding function
        print(f"Loading embedding model: {self.embedding_model_name}...")
        self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=self.embedding_model_name,
            device="cpu"
        )
        
        # Get the collection
        try:
            self.collection = self.client_db.get_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function
            )
            print(f"‚úì Connected to collection '{self.collection_name}' with {self.collection.count()} documents.\n")
        except Exception as e:
            print(f"Error: Could not load collection. {e}")
            raise e

    def search(self, query: str, top_k: int = 6) -> List[Dict[str, Any]]:
        """Search for relevant legal text chunks."""
        print(f"üîç Searching for: '{query}'")
        
        results = self.collection.query(
            query_texts=[query],
            n_results=top_k
        )
        
        formatted_results = []
        for i in range(len(results['ids'][0])):
            formatted_results.append({
                'rank': i + 1,
                'id': results['ids'][0][i],
                'text': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i] if 'distances' in results else None
            })
        
        print(f"‚úì Found {len(formatted_results)} relevant legal chunks.\n")
        return formatted_results

    def generate_system_prompt(self) -> str:
        """Generate system prompt that enforces strict context-only answers."""
        return """You are "‡§µ‡§ï‡§ø‡§≤" - an expert Nepali legal advisor AI assistant. You help users understand Nepali law by providing accurate, well-structured answers.

CRITICAL RULES (YOU MUST FOLLOW STRICTLY):

1. **Language**: Answer ONLY in NEPALI (‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§≠‡§æ‡§∑‡§æ), even if the question is in English

2. **Source Grounding**: Use ONLY the legal context provided in the user message. DO NOT use your training data or general knowledge. If the context doesn't contain the answer, clearly state: "‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§¶‡§ø‡§á‡§è‡§ï‡•ã ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠‡§Æ‡§æ ‡§Ø‡§∏ ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§ï‡•ã ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§ú‡§µ‡§æ‡§´ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§õ‡•à‡§®‡•§"

3. **Citations**: For every important legal point, provide clear citations:
   - Format: "‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®, ‡§ß‡§æ‡§∞‡§æ ‡•≠‡•¨ ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞..."
   - Or: "‡§®‡§æ‡§ó‡§∞‡§ø‡§ï ‡§∏‡§Ç‡§π‡§ø‡§§‡§æ, ‡•®‡•¶‡•≠‡•™, ‡§¶‡§´‡§æ ‡•ß‡•≠ ‡§¨‡§Æ‡•ã‡§ú‡§ú‡§Æ..."

4. **Professional Formatting**:
   - Start with a brief introduction
   - Use bullet points (‚Ä¢) or numbered lists (‡•ß., ‡•®., ‡•©.)
   - Use relevant emojis for clarity: ‚öñÔ∏è (law), üìã (rules), ‚úÖ (allowed), ‚ùå (prohibited), üìä (process)
   - Use headers with ** ** for sections
   - End with "üìö ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠:" section listing sources

5. **Structure Example**:
```
[Brief introduction answering the core question]

**‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§¨‡•Å‡§Å‡§¶‡§æ‡§π‡§∞‡•Ç:**

‡•ß. **[Topic 1]:** 
   ‚Ä¢ [Point with citation]
   ‚Ä¢ [Point with citation]

‡•®. **[Topic 2]:**
   ‚Ä¢ [Point with citation]

**‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ:** [If applicable]
‚Ä¢ [Step-by-step process]

üìö **‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠:**
‚Ä¢ ‡§®‡•á‡§™‡§æ‡§≤‡§ï‡•ã ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§®, ‡§ß‡§æ‡§∞‡§æ X
‚Ä¢ [Other sources used]
```

6. **Accuracy**: Be precise and detailed. Don't generalize or assume.

Remember: Your credibility depends on ONLY using the provided legal context. Never hallucinate or add information not in the context."""

    def generate_user_prompt(self, query: str, retrieved_chunks: List[Dict[str, Any]], max_chunk_chars: int = 800) -> str:
        """Generate user prompt with query and legal context.
        
        Args:
            query: User's question
            retrieved_chunks: Retrieved legal chunks
            max_chunk_chars: Maximum characters per chunk (default: 800 to stay under token limits)
        """
        
        # Build the context from retrieved chunks
        context_parts = []
        for i, chunk in enumerate(retrieved_chunks, 1):
            citation_ref = chunk['metadata'].get('citation_reference', 'Unknown Source')
            section_type = chunk['metadata'].get('section_type_ne', '')
            section_num = chunk['metadata'].get('section_number', '')
            text = chunk['text']
            
            # Truncate text if too long
            if len(text) > max_chunk_chars:
                text = text[:max_chunk_chars] + "..."
            
            header = f"[‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠ {i}: {citation_ref}"
            if section_type and section_num:
                header += f", {section_type} {section_num}"
            header += "]"
            
            context_parts.append(f"{header}\n{text}")
        
        context = "\n\n---\n\n".join(context_parts)
        
        # Construct the user message
        user_message = f"""**‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ‡§ï‡•ã ‡§™‡•ç‡§∞‡§∂‡•ç‡§®:**
{query}

**‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠ (‡§Ø‡•ã ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç):**

{context}

---

‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§æ‡§•‡§ø‡§ï‡•ã ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§∏‡§®‡•ç‡§¶‡§∞‡•ç‡§≠‡§ï‡•ã ‡§Ü‡§ß‡§æ‡§∞‡§Æ‡§æ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ‡§ï‡•ã ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§ï‡•ã ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§, ‡§∏‡§Ç‡§∞‡§ö‡§ø‡§§ ‡§∞ ‡§™‡•á‡§∂‡•á‡§µ‡§∞ ‡§ú‡§µ‡§æ‡§´ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä‡§Æ‡§æ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§"""
        
        return user_message

    def query_groq(self, system_prompt: str, user_prompt: str) -> str:
        """Send prompt to Groq and get streaming response."""
        
        print("ü§ñ Generating answer using Groq (ultra-fast)...")
        print("=" * 60)
        print("üìñ ‡§ú‡§µ‡§æ‡§´ (‡§µ‡§ï‡§ø‡§≤):")
        print("=" * 60 + "\n")
        
        try:
            # Create chat completion with streaming
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,  # Low temperature for factual accuracy
                max_tokens=2048,
                top_p=0.9,
                stream=True
            )
            
            full_response = ""
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    print(content, end='', flush=True)
                    full_response += content
            
            print("\n\n" + "=" * 60)
            return full_response
                
        except Exception as e:
            error_msg = f"‚ùå Error with Groq API: {str(e)}"
            print(error_msg)
            return error_msg

    def is_greeting_or_general(self, query: str) -> tuple[bool, str]:
        """Check if query is a greeting or general question about the bot."""
        query_lower = query.lower().strip()
        
        # Common greetings in English and Nepali
        greetings = ['hi', 'hello', 'hey', '‡§®‡§Æ‡§∏‡•ç‡§§‡•á', '‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞', '‡§π‡§æ‡§Ø', '‡§π‡•á‡§≤‡•ã']
        
        # Bot capability questions
        capability_keywords = [
            'what can you do', 'how can you help', 'who are you', 'what are you',
            '‡§§‡§™‡§æ‡§à ‡§ï‡•á ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ', '‡§§‡§™‡§æ‡§à ‡§ï‡•ã ‡§π‡•ã', '‡§§‡§™‡§æ‡§à ‡§ï‡•á ‡§π‡•ã', '‡§Æ‡§¶‡§§ ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ',
            'help me', '‡§Æ‡§≤‡§æ‡§à ‡§Æ‡§¶‡§§ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç'
        ]
        
        # Check for simple greetings
        if query_lower in greetings or len(query.split()) <= 2 and any(g in query_lower for g in greetings):
            return True, """‡§®‡§Æ‡§∏‡•ç‡§§‡•á! üôè

‡§Æ **‡§ì‡§ï‡§ø‡§≤ AI** ‡§π‡•Å‡§Å, ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§∏‡§π‡§Ø‡•ã‡§ó‡•Ä‡•§ 

**‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§ï‡§∏‡§∞‡•Ä ‡§Æ‡§¶‡§§ ‡§ó‡§∞‡•ç‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å:**

‚Ä¢ **‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§® ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§®**: ‡§Æ‡•å‡§≤‡§ø‡§ï ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞, ‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ, ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§§‡§æ ‡§Ü‡§¶‡§ø
‚Ä¢ **‡§¶‡•Ä‡§µ‡§æ‡§®‡•Ä ‡§∏‡§Ç‡§π‡§ø‡§§‡§æ**: ‡§∏‡§Æ‡•ç‡§™‡§§‡•ç‡§§‡§ø, ‡§µ‡§ø‡§µ‡§æ‡§π, ‡§â‡§§‡•ç‡§§‡§∞‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞, ‡§ã‡§£ ‡§Ü‡§¶‡§ø
‚Ä¢ **‡§ú‡§ó‡•ç‡§ó‡§æ ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡•Ä ‡§ï‡§æ‡§®‡•Ç‡§®**: ‡§ú‡§ó‡•ç‡§ó‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó, ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ, ‡§ú‡§ó‡•ç‡§ó‡§æ ‡§ï‡§∞ ‡§Ü‡§¶‡§ø
‚Ä¢ **‡§∏‡§Æ‡•ç‡§™‡§§‡•ç‡§§‡§ø ‡§ï‡§∞**: ‡§∏‡§Æ‡•ç‡§™‡§§‡•ç‡§§‡§ø ‡§ï‡§∞ ‡§®‡§ø‡§Ø‡§Æ, ‡§¶‡§∞ ‡§∞ ‡§≠‡•Å‡§ï‡•ç‡§§‡§æ‡§®‡•Ä ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ
‚Ä¢ **‡§µ‡§ø‡§§‡•ç‡§§‡•Ä‡§Ø ‡§ê‡§®**: ‡§¨‡§ú‡•á‡§ü, ‡§ï‡§∞, ‡§∞‡§æ‡§ú‡§∏‡•ç‡§µ ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡•Ä ‡§®‡§ø‡§Ø‡§Æ‡§π‡§∞‡•Ç

‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§≤‡§æ‡§à ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§∏‡§ü‡•Ä‡§ï ‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§∏‡§®‡•Ä‡§Ø ‡§ú‡§µ‡§æ‡§´ ‡§¶‡§ø‡§® ‡§§‡§Ø‡§æ‡§∞ ‡§õ‡•Å‡•§ üíº‚öñÔ∏è"""
        
        # Check for capability questions
        if any(keyword in query_lower for keyword in capability_keywords):
            return True, """‡§®‡§Æ‡§∏‡•ç‡§§‡•á! üôè

‡§Æ **‡§ì‡§ï‡§ø‡§≤ AI** ‡§π‡•Å‡§Å - ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§ï‡§æ‡§®‡•Ç‡§®‡§ï‡•ã ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§ï‡•É‡§§‡•ç‡§∞‡§ø‡§Æ ‡§¨‡•Å‡§¶‡•ç‡§ß‡§ø‡§Æ‡§§‡•ç‡§§‡§æ‡•§ 

**‡§Æ‡•á‡§∞‡§æ ‡§ï‡•ç‡§∑‡§Æ‡§§‡§æ‡§π‡§∞‡•Ç:**

**‡•ß. ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§® ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä:**
‚Ä¢ ‡§Æ‡•å‡§≤‡§ø‡§ï ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§∞ ‡§ï‡§∞‡•ç‡§§‡§µ‡•ç‡§Ø
‚Ä¢ ‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§∏‡§Ç‡§∞‡§ö‡§®‡§æ ‡§∞ ‡§∂‡§ï‡•ç‡§§‡§ø ‡§µ‡§ø‡§≠‡§æ‡§ú‡§®
‚Ä¢ ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§§‡§æ ‡§∞ ‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§ö‡§® ‡§™‡•ç‡§∞‡§£‡§æ‡§≤‡•Ä

**‡•®. ‡§¶‡•Ä‡§µ‡§æ‡§®‡•Ä ‡§ï‡§æ‡§®‡•Ç‡§®:**
‚Ä¢ ‡§µ‡§ø‡§µ‡§æ‡§π, ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡§µ‡§ø‡§ö‡•ç‡§õ‡•á‡§¶ ‡§∞ ‡§™‡§æ‡§∞‡§ø‡§µ‡§æ‡§∞‡§ø‡§ï ‡§Æ‡§æ‡§Æ‡§ø‡§≤‡§æ
‚Ä¢ ‡§∏‡§Æ‡•ç‡§™‡§§‡•ç‡§§‡§ø ‡§∞ ‡§â‡§§‡•ç‡§§‡§∞‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞
‚Ä¢ ‡§ã‡§£, ‡§ï‡§∞‡•ç‡§ú‡§æ ‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§™‡§æ‡§∞ ‡§∏‡§Æ‡•ç‡§ù‡•å‡§§‡§æ

**‡•©. ‡§ú‡§ó‡•ç‡§ó‡§æ ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡•Ä ‡§ï‡§æ‡§®‡•Ç‡§®:**
‚Ä¢ ‡§ú‡§ó‡•ç‡§ó‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§∞ ‡§∏‡•ç‡§µ‡§æ‡§Æ‡§ø‡§§‡•ç‡§µ
‚Ä¢ ‡§ú‡§ó‡•ç‡§ó‡§æ ‡§ï‡§∞ ‡§∞ ‡§¶‡§∞‡•ç‡§§‡§æ ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ

**‡•™. ‡§ï‡§∞ ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡•Ä ‡§®‡§ø‡§Ø‡§Æ:**
‚Ä¢ ‡§∏‡§Æ‡•ç‡§™‡§§‡•ç‡§§‡§ø ‡§ï‡§∞, ‡§Ü‡§Ø‡§ï‡§∞
‚Ä¢ ‡§ï‡§∞ ‡§≠‡•Å‡§ï‡•ç‡§§‡§æ‡§®‡•Ä ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ

**‡•´. ‡§µ‡§ø‡§§‡•ç‡§§‡•Ä‡§Ø ‡§ê‡§® ‡§∞ ‡§¨‡§ú‡•á‡§ü**

‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§≤‡§æ‡§à ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§∏‡§ü‡•Ä‡§ï, ‡§∏‡§Ç‡§∞‡§ö‡§ø‡§§ ‡§∞ ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡§ø‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ó‡§∞‡•ç‡§®‡•á‡§õ‡•Å‡•§ ‚öñÔ∏è"""
        
        return False, ""
    
    def is_off_topic(self, query: str) -> tuple[bool, str]:
        """Check if query is completely off-topic (non-legal)."""
        query_lower = query.lower().strip()
        
        # Non-legal topics
        off_topic_keywords = [
            'weather', '‡§Æ‡•å‡§∏‡§Æ', 'recipe', '‡§ñ‡§æ‡§®‡§æ ‡§™‡§ï‡§æ‡§â‡§®‡•á', 'cooking', 'sports', '‡§ñ‡•á‡§≤‡§ï‡•Å‡§¶',
            'movie', 'film', '‡§ö‡§≤‡§ö‡§ø‡§§‡•ç‡§∞', 'music', '‡§∏‡§Ç‡§ó‡•Ä‡§§', 'game', '‡§ñ‡•á‡§≤',
            'celebrity', '‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø', 'joke', '‡§Æ‡§ú‡§æ‡§ï', 'story', '‡§ï‡§•‡§æ',
            'math', '‡§ó‡§£‡§ø‡§§', 'science', '‡§µ‡§ø‡§ú‡•ç‡§û‡§æ‡§®', 'history', '‡§á‡§§‡§ø‡§π‡§æ‡§∏',
            'geography', '‡§≠‡•Ç‡§ó‡•ã‡§≤', 'astronomy', '‡§ñ‡§ó‡•ã‡§≤', 'medicine', '‡§î‡§∑‡§ß‡§ø'
        ]
        
        # Check for off-topic keywords
        if any(keyword in query_lower for keyword in off_topic_keywords):
            return True, """‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç! üôè

‡§Æ **‡§ì‡§ï‡§ø‡§≤ AI** - ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑‡§ú‡•ç‡§û ‡§π‡•Å‡§Å‡•§ ‡§Æ ‡§ï‡•á‡§µ‡§≤ **‡§ï‡§æ‡§®‡•Ç‡§® ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§π‡§∞‡•Ç‡§ï‡•ã** ‡§ú‡§µ‡§æ‡§´ ‡§¶‡§ø‡§® ‡§∏‡§ï‡•ç‡§õ‡•Å‡•§

**‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§≤‡§æ‡§à ‡§Ø‡§∏‡•ç‡§§‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç:**
‚Ä¢ ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§® ‡§∞ ‡§Æ‡•å‡§≤‡§ø‡§ï ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞
‚Ä¢ ‡§¶‡•Ä‡§µ‡§æ‡§®‡•Ä ‡§ï‡§æ‡§®‡•Ç‡§® (‡§µ‡§ø‡§µ‡§æ‡§π, ‡§∏‡§Æ‡•ç‡§™‡§§‡•ç‡§§‡§ø, ‡§â‡§§‡•ç‡§§‡§∞‡§æ‡§ß‡§ø‡§ï‡§æ‡§∞)
‚Ä¢ ‡§ú‡§ó‡•ç‡§ó‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§∞ ‡§ú‡§ó‡•ç‡§ó‡§æ ‡§ï‡§∞
‚Ä¢ ‡§∏‡§Æ‡•ç‡§™‡§§‡•ç‡§§‡§ø ‡§ï‡§∞ ‡§∞ ‡§µ‡§ø‡§§‡•ç‡§§‡•Ä‡§Ø ‡§ê‡§®
‚Ä¢ ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§§‡§æ ‡§∞ ‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§ö‡§®

‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§∏‡§π‡§Ø‡•ã‡§ó‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§Æ ‡§§‡§Ø‡§æ‡§∞ ‡§õ‡•Å! ‚öñÔ∏èüíº"""
        
        return False, ""

    def answer_question(self, query: str, top_k: int = 6) -> Dict[str, Any]:
        """Complete RAG pipeline: retrieve relevant chunks and generate answer."""
        
        # Step 0: Check for greetings or general questions
        is_greeting, greeting_response = self.is_greeting_or_general(query)
        if is_greeting:
            return {
                'query': query,
                'answer': greeting_response,
                'sources': []
            }
        
        # Step 0.5: Check for off-topic questions
        is_off_topic, off_topic_response = self.is_off_topic(query)
        if is_off_topic:
            return {
                'query': query,
                'answer': off_topic_response,
                'sources': []
            }
        
        # Step 1: Retrieve relevant chunks
        results = self.search(query, top_k=top_k)
        
        if not results:
            return {
                'query': query,
                'answer': '‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§∏‡§Å‡§ó ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡§ø‡§§ ‡§ï‡•Å‡§®‡•à ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§',
                'sources': []
            }
        
        # Step 2: Generate prompts
        system_prompt = self.generate_system_prompt()
        user_prompt = self.generate_user_prompt(query, results)
        
        # Step 3: Get answer from Groq
        answer = self.query_groq(system_prompt, user_prompt)
        
        # Step 4: Display sources
        print(f"\nüìö ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡§ø‡§è‡§ï‡§æ ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§∏‡•ç‡§∞‡•ã‡§§‡§π‡§∞‡•Ç ({len(results)} ‡§Ö‡§Ç‡§∂):")
        print("-" * 60)
        for i, source in enumerate(results, 1):
            citation = source['metadata'].get('citation_reference', 'Unknown')
            section_type = source['metadata'].get('section_type_ne', '')
            section_num = source['metadata'].get('section_number', '')
            if section_type and section_num:
                print(f"   {i}. {citation} ({section_type} {section_num})")
            else:
                print(f"   {i}. {citation}")
        print()
        
        return {
            'query': query,
            'answer': answer,
            'sources': results,
            'system_prompt': system_prompt,
            'user_prompt': user_prompt
        }


def main():
    """Interactive Q&A session with Groq."""
    print("=" * 60)
    print("‚ö° LEGAL RAG SYSTEM WITH GROQ API")
    print("=" * 60)
    print("‚ú® Ultra-fast responses (< 5 seconds)")
    print("‚ú® Cross-lingual: Ask in English, get Nepali answers")
    print("‚ú® Powered by: Llama 3.1 70B + ChromaDB")
    print("=" * 60 + "\n")
    
    try:
        # Initialize RAG system
        rag = LegalRAGWithGroq()
        
        print("üí° Example queries:")
        print("   ‚Ä¢ How is the prime minister elected in Nepal?")
        print("   ‚Ä¢ What are the fundamental rights?")
        print("   ‚Ä¢ ‡§ú‡§ó‡•ç‡§ó‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡•ã ‡§®‡§ø‡§Ø‡§Æ ‡§ï‡•á ‡§õ?")
        print("   ‚Ä¢ ‡§∏‡§Æ‡•ç‡§™‡§§‡•ç‡§§‡§ø ‡§ï‡§∞ ‡§ï‡§∏‡§∞‡•Ä ‡§§‡§ø‡§∞‡•ç‡§®‡•á?")
        print("   ‚Ä¢ ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§§‡§æ‡§ï‡•ã ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§ï‡•á ‡§õ‡§®‡•ç?")
        print("   ‚Ä¢ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø‡§ï‡•ã ‡§∂‡§ï‡•ç‡§§‡§ø ‡§∞ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ ‡§ï‡•á ‡§π‡•Å‡§®‡•ç?")
        
        print(f"\n‚úÖ Ready for questions! (Type 'quit' to exit)")
        print("=" * 60)
        
        while True:
            try:
                query = input("\n‚ùì Your question: ").strip()
                
                if query.lower() in ['quit', 'exit', 'q']:
                    print("\nüëã ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! ‡§´‡•á‡§∞‡§ø ‡§≠‡•á‡§ü‡•å‡§Ç‡§≤‡§æ‡•§")
                    break
                
                if not query:
                    continue
                
                # Get answer
                result = rag.answer_question(query, top_k=6)
                
            except KeyboardInterrupt:
                print("\n\nüëã ‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶! Goodbye!")
                break
            except Exception as e:
                print(f"\n‚ùå Error: {e}")
                import traceback
                traceback.print_exc()
    
    except Exception as e:
        print(f"Failed to initialize system: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
